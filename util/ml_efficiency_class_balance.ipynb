{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"..\")\n",
    "from src.dataset import Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, RepeatedKFold\n",
    "from sklearn.exceptions import NotFittedError\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "import xgboost as xgb\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_dataset_generator = Dataset(\"adult\")\n",
    "all_data = adult_dataset_generator.original_dataframe.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_test, y_pred, problem):\n",
    "    return [m(y_test, y_pred) for m in problem[\"metrics\"]]\n",
    "    \n",
    "def train_eval(X_train, y_train, X_test, y_test, problem):\n",
    "    model = problem[\"model\"](**problem[\"args\"])\n",
    "    try:\n",
    "        model.predict(X_test)\n",
    "        print(name_tr, name_t, \"Model is already fitted!\")\n",
    "        exit(1)\n",
    "    except:\n",
    "        pass\n",
    "    # object_columns = X_train.select_dtypes(include=['object']).columns\n",
    "    # X_train[object_columns] = X_train[object_columns].astype(str)\n",
    "    # X_test[object_columns] = X_test[object_columns].astype(str)\n",
    "    # print(object_columns)\n",
    "    # print(X_train.nunique())\n",
    "    # print(y_train.nunique())\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    metrics = compute_metrics(y_test, y_pred, problem)\n",
    "    return metrics, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(problem_classification, adult_dataset_generator, all_data, num_repeats = 1, num_folds = 2, protected_attributes = [\"sex\"]):\n",
    "\n",
    "    average_problems = []\n",
    "    std_problems = []\n",
    "    for problem in problems_classification:\n",
    "\n",
    "        rkf = RepeatedKFold(n_splits=num_folds, n_repeats=num_repeats, random_state=42)\n",
    "        all_metrics_mean = []\n",
    "        all_metrics_std = []\n",
    "        metrics_all = []\n",
    "        for i, (train_index, test_index) in enumerate(rkf.split(all_data)):    \n",
    "\n",
    "            data_train, data_test = all_data.loc[train_index], all_data.loc[test_index]\n",
    "            data_train_encoded = adult_dataset_generator.encode(data_train, keep_dtypes=True)\n",
    "            data_test_encoded = adult_dataset_generator.encode(data_test)\n",
    "\n",
    "\n",
    "            X_train_real = data_train.copy().drop(columns=[\"income\"])\n",
    "            y_train_real = data_train_encoded[\"income\"].copy().astype(\"int\")\n",
    "\n",
    "            test_sets, _ = adult_dataset_generator.split_population(data_test)\n",
    "            test_sets[\"all\"] = data_test\n",
    "\n",
    "            split_dfs, additional_sizes = adult_dataset_generator.split_population(data_train, protected_attributes=protected_attributes)\n",
    "\n",
    "\n",
    "            # Get the DataFrame with the maximum length\n",
    "            max_length_df_key = max(split_dfs, key=lambda x: len(split_dfs[x]))\n",
    "            # Retrieve the DataFrame using the key\n",
    "            max_length_df = split_dfs[max_length_df_key]\n",
    "\n",
    "            max_length_df_class_counts = max_length_df['income'].value_counts()\n",
    "\n",
    "            max_length_df_majority_class = max_length_df_class_counts.idxmax()\n",
    "            max_length_df_majority_class_count = max_length_df_class_counts[max_length_df_majority_class]\n",
    "\n",
    "            augmented_dfs = []\n",
    "            split_df_keys, split_df_vals = zip(*split_dfs.items())\n",
    "\n",
    "            for split_key, split_df in split_dfs.items():\n",
    "                class_counts = split_df['income'].value_counts()\n",
    "                augmented_dfs.append(split_df)\n",
    "\n",
    "                for class_label, class_count in class_counts.items():\n",
    "                    minority_class_count = class_count\n",
    "                    imbalance = max_length_df_majority_class_count - minority_class_count\n",
    "                    size = imbalance\n",
    "\n",
    "                    if size > 0:\n",
    "                        class_split_df = split_df[split_df['income'] == class_label].copy()\n",
    "                        class_split_df.drop('income', axis=1, inplace=True)\n",
    "                        class_split_df.drop('sex', axis=1, inplace=True)\n",
    "\n",
    "                        split_synthesizer = adult_dataset_generator.train_synthesizer(class_split_df, encode=True) \n",
    "                        split_synthetic_data = adult_dataset_generator.generate_data(split_synthesizer, num=size)\n",
    "                        split_synthetic_data['income'] = class_label\n",
    "                        split_synthetic_data['sex'] = split_key\n",
    "                        augmented_dfs.append(split_synthetic_data.copy())\n",
    "\n",
    "            augmented_trainingset = pd.concat(augmented_dfs)\n",
    "            augmented_trainingset_encoded = adult_dataset_generator.encode(augmented_trainingset, keep_dtypes=True)\n",
    "\n",
    "            X_train_augmented = augmented_trainingset.drop(columns=[\"income\"])\n",
    "            y_train_augmented = augmented_trainingset_encoded[\"income\"].astype(\"int\")\n",
    "\n",
    "            train_real = data_train_encoded[\"income\"].astype(\"int\")\n",
    "\n",
    "\n",
    "            train_sets_X = [X_train_real, X_train_augmented]\n",
    "            train_sets_y = [y_train_real, y_train_augmented]\n",
    "            metrics_split = []\n",
    "            \n",
    "            for X_train, y_train in zip(train_sets_X, train_sets_y):\n",
    "                setup_metrics = []\n",
    "                preds = [] \n",
    "                for test_set_name, test_set in test_sets.items():\n",
    "                    test_set_encoded = adult_dataset_generator.encode(test_set)\n",
    "                    X_test = test_set.drop(columns=[\"income\"])\n",
    "                    y_test = test_set_encoded[\"income\"].astype(\"int\")\n",
    "\n",
    "                    results, pred = train_eval(X_train, y_train, X_test, y_test, problem)\n",
    "                    setup_metrics.append(results)\n",
    "                    preds.append(pred)\n",
    "                metrics_split.append(setup_metrics)\n",
    "            metrics_all.append(metrics_split)\n",
    "        metrics_all = np.array(metrics_all)    \n",
    "        average_metrics_all = np.mean(metrics_all, axis=0)\n",
    "        std_metrics_all = np.std(metrics_all, axis=0)\n",
    "        average_problems.append(average_metrics_all)\n",
    "        std_problems.append(std_metrics_all)\n",
    "    return np.array(average_problems), np.array(std_problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_classification = {\"metrics\":[accuracy_score,  precision_score, recall_score, f1_score],\n",
    "                      \"metric_names\":[\"Accuracy\", \"P\", \"R\", \"F1\"]}\n",
    "                      \n",
    "# models = [MultiOutputRegressor(LGBMRegressor(random_state=42)), DecisionTreeRegressor(random_state=42), RandomForestRegressor(random_state=42)]\n",
    "# models_classification = [xgb.XGBClassifier, CatBoostClassifier, DecisionTreeClassifier, RandomForestClassifier]\n",
    "# models_classification = [xgb.XGBClassifier]\n",
    "models_classification = [CatBoostClassifier]\n",
    "\n",
    "# args = [{\"random_state\":42}, {\"random_state\":42, \"loss_function\":\"MultiRMSE\", \"verbose\":False, \"iterations\":100, \"learning_rate\":0.01}, {\"random_state\":42}, {\"random_state\":42}]\n",
    "args = [{\"random_state\":42, \"loss_function\":\"Logloss\", \"verbose\":False, \"iterations\":100, \"learning_rate\":0.01, \"cat_features\":adult_dataset_generator.categorical_input_cols}]\n",
    "\n",
    "# model_names_classification = [\"xgboost\", \"catboost\", \"DT\", \"RF\"]\n",
    "model_names_classification = [\"catboost\"]\n",
    "problems_classification = []\n",
    "for model, name, arg in zip(models_classification, model_names_classification, args):\n",
    "    problem = problem_classification.copy()\n",
    "    problem[\"model\"] = copy.deepcopy(model)\n",
    "    problem[\"model_name\"] = name\n",
    "    problem[\"args\"] = arg\n",
    "    problems_classification.append(problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "average, std = run_experiments(problem_classification, adult_dataset_generator, all_data, num_repeats = 5, num_folds = 3, protected_attributes = [\"sex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latex_table1(all_metrics_mean, all_metrics_std, names_train, names_test, problems, test_data=False, metric_names_actual=[]):\n",
    "    if test_data:\n",
    "        all_cols =  str(2 + len(metric_names_actual) * len(names_test))\n",
    "    else:\n",
    "        all_cols = str(len(problems[0][\"metric_names\"]) + 2)\n",
    "    latex_table = \"\\\\begin{table}[h]\\n\"\n",
    "    latex_table += \"\\\\centering\\n\"\n",
    "    # latex_table += \"\\\\scalebox{0.70}{\\n\"\n",
    "    latex_table += \"\\\\begin{tabular}{l l \" + \" \".join([\"c\"]*(int(all_cols)-2)) + \"}\\n\"\n",
    "    latex_table += \"\\\\hline\\n\"\n",
    "    if test_data:\n",
    "        if len(metric_names_actual) > 0:\n",
    "            latex_table += \"Model & Train data & \\multicolumn{\" + str(len(names_test) * len(metric_names_actual)) + \"}{c}{Test data} \\\\\\\\\\n\" \n",
    "            latex_table += \"& \"\n",
    "            for name_t in names_test:\n",
    "                latex_table +=\" & \\multicolumn{\" + str(len(metric_names_actual)) + \"}{c}{\" + name_t + \"}\"\n",
    "            latex_table += \"\\\\\\\\\\n\"\n",
    "            latex_table += \"\\cline{3-\" + str(all_cols) +\"}\"\n",
    "            # latex_table +=  \"& & \" + \" & \".join(metric_names_actual) + \" & \" + \" & \".join(metric_names_actual) + \" \\\\\\\\\\n\"\n",
    "            latex_table +=  \"& \" + \"\".join([\" & \" + \" & \".join(metric_names_actual) for _ in range(len(names_test))]) + \" \\\\\\\\\\n\"\n",
    "        else:\n",
    "            latex_table += \"Model & Train data & Test data & \" + \" & \".join(problems[0][\"metric_names\"]) + \" \\\\\\\\\\n\"\n",
    "    else:\n",
    "        if len(metric_names_actual) > 0:\n",
    "            latex_table += \"Model & Train data & \" + \" & \".join(metric_names_actual) + \" \\\\\\\\\\n\"\n",
    "        else:\n",
    "            latex_table += \"Model & Train data & \" + \" & \".join(problems[0][\"metric_names\"]) + \" \\\\\\\\\\n\"\n",
    "\n",
    "    latex_table += \"\\\\hline\"\n",
    "    count_make_cell = sum(\"makecell\" in item for item in names_train)\n",
    "\n",
    "    for problem_i in range(len(problems)):\n",
    "\n",
    "        latex_table += \"\\\\multirow{\" + str(2*len(problems)) + \"}{*}{\" + problems[problem_i][\"model_name\"] + \"}\"\n",
    "\n",
    "        for i in range(len(names_train)):\n",
    "            train_name = names_train[i]\n",
    "            # if \"makecell\" in train_name:\n",
    "            #     latex_table += \" & \" + \"\\\\multirow{2}{*}{\" + train_name + \"}\"\n",
    "            # else:\n",
    "            latex_table += \" & \" + \"\\\\multirow{2}{*}{\" + train_name + \"}\"\n",
    "            # avg_metric = all_metrics_mean[metric_row][name_row][metric_col]\n",
    "            # std_metric = all_metrics_std[metric_row][name_row][metric_col]\n",
    "            # latex_table += f\"& {avg_metric:.3f} ({std_metric:.3f})\"\n",
    "            avgs_c = \"\"\n",
    "            stds_c = \"\"\n",
    "            for j in range(len(names_test)):\n",
    "                test_name = names_test[j]\n",
    "                avg_metric = all_metrics_mean[problem_i][i][j]\n",
    "                std_metric = all_metrics_std[problem_i][i][j]\n",
    "                # std_metric = all_metrics_std[metric_row][name_row][metric_col]\n",
    "                avgs_c += \" & \" + \" & \".join(map(lambda x: \"{:.3f}\".format(x), avg_metric))\n",
    "                stds_c += \" & \" + \" & \".join(map(lambda x: \"({:.3f})\".format(x), std_metric))\n",
    "\n",
    "                # if test_data:\n",
    "                #     latex_table += \" & \" + test_name + \" & \" +  numbers + \" \\\\\\\\\\n\"\n",
    "                # else:\n",
    "            latex_table += avgs_c + \" \\\\\\\\\\n\"\n",
    "            latex_table += \" & \" + stds_c + \" \\\\\\\\\\n\"\n",
    "\n",
    "                # latex_table += \"\\\\cline{2-\" + all_cols + \"}\\n\"\n",
    "        latex_table += \"\\\\hline\\n\"\n",
    "    latex_table += \"\\\\end{tabular}\\n\"\n",
    "    # latex_table += \"}\\n\"\n",
    "    latex_table += \"\\\\caption{Comparison}\\n\"\n",
    "    latex_table += \"\\\\label{tab:eval}\\n\"\n",
    "    latex_table += \"\\\\end{table}\"\n",
    "    \n",
    "    return latex_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sex=Female', 'Sex=Male']\n",
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\begin{tabular}{l l c c c c c c c c c c c c}\n",
      "\\hline\n",
      "Model & Train data & \\multicolumn{12}{c}{Test data} \\\\\n",
      "&  & \\multicolumn{4}{c}{Sex=Female} & \\multicolumn{4}{c}{Sex=Male} & \\multicolumn{4}{c}{Overall}\\\\\n",
      "\\cline{3-14}&  & Accuracy & P & R & F1 & Accuracy & P & R & F1 & Accuracy & P & R & F1 \\\\\n",
      "\\hline\\multirow{2}{*}{catboost} & \\multirow{2}{*}{Adult} & 0.915 & 0.887 & 0.261 & 0.403 & 0.770 & 0.906 & 0.277 & 0.424 & 0.818 & 0.903 & 0.275 & 0.421 \\\\\n",
      " &  & (0.002) & (0.022) & (0.017) & (0.021) & (0.007) & (0.013) & (0.011) & (0.013) & (0.005) & (0.013) & (0.009) & (0.012) \\\\\n",
      " & \\multirow{2}{*}{Augmented Adult} & 0.870 & 0.449 & 0.809 & 0.577 & 0.711 & 0.517 & 0.865 & 0.647 & 0.764 & 0.506 & 0.857 & 0.636 \\\\\n",
      " &  & (0.011) & (0.024) & (0.017) & (0.018) & (0.006) & (0.010) & (0.006) & (0.007) & (0.005) & (0.009) & (0.006) & (0.007) \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\caption{Comparison}\n",
      "\\label{tab:eval}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "metric_names_actual = [\"Accuracy\", \"P\", \"R\", \"F1\"]\n",
    "names_train = [\"Adult\", \"Augmented Adult\"]\n",
    "test_sets, _ = adult_dataset_generator.split_population(all_data)\n",
    "protected_attributes = [\"Sex\"]\n",
    "# names_test = [\"\\\\makecell[c]{\" + '\\\\\\\\'.join([f\"{attr}-{value}\" for attr, value in zip(protected_attributes, entry)]) + \"}\" for entry in test_sets.keys()]\n",
    "# names_test = [' \\& '.join([f\"{attr}={value}\" for attr, value in zip(protected_attributes, entry)]) for entry in test_sets.keys()]\n",
    "names_test = [f\"Sex={value}\" for value in test_sets.keys()]\n",
    "print(names_test)\n",
    "names_test.append(\"Overall\")\n",
    "latex_table = generate_latex_table1(average, std, names_train, names_test, problems_classification, metric_names_actual=metric_names_actual, test_data=True)\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Female', 'Male'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sets.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samplestructures",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
