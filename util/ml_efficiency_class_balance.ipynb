{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"..\")\n",
    "from src.dataset import Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, RepeatedKFold\n",
    "from sklearn.exceptions import NotFittedError\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import demographic_parity_difference, demographic_parity_ratio, true_positive_rate_difference, true_positive_rate, false_positive_rate_difference\n",
    "\n",
    "def eq_odd(y_test, y_pred, group_test):\n",
    "    return true_positive_rate_difference(y_test, y_pred, sensitive_features=group_test)\\\n",
    "                + false_positive_rate_difference(y_test, y_pred, sensitive_features=group_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "import xgboost as xgb\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.base import BaseEstimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_dataset_generator = Dataset(\"adult\")\n",
    "all_data = adult_dataset_generator.original_dataframe.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_test, y_pred, problem):\n",
    "    return [m(y_test, y_pred) for m in problem[\"metrics\"]]\n",
    "\n",
    "def compute_fairness_metrics(y_test, y_pred, problem, group):\n",
    "    return [m(y_test, y_pred, group) for m in problem[\"fairness_metrics\"]]    \n",
    "\n",
    "def is_fitted(estimator, X_test):\n",
    "    if isinstance(estimator, Pipeline):\n",
    "        for _, step in estimator.steps:\n",
    "            try:\n",
    "                step.__dict__['estimators_']\n",
    "            except (AttributeError, KeyError):\n",
    "                return False\n",
    "    elif isinstance(estimator, BaseEstimator):\n",
    "        try:\n",
    "            estimator.__dict__['estimators_']\n",
    "        except (AttributeError, KeyError):\n",
    "            return False\n",
    "    else:\n",
    "        try:\n",
    "            estimator.predict(X_test)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def train_eval(X_train, y_train, X_test, y_test, problem, keep_protected_input=False):\n",
    "\n",
    "    if not keep_protected_input:\n",
    "        X_train_copy = X_train.copy()\n",
    "        X_test_copy = X_test.copy()\n",
    "        group_test = X_test_copy[\"sex\"].copy()\n",
    "\n",
    "        X_train = X_train_copy.drop('sex', axis=1)\n",
    "        X_test = X_test_copy.drop('sex', axis=1)\n",
    "        if \"cat_features\" in problem[\"args\"]:\n",
    "            cflist = [item for item in problem[\"args\"][\"cat_features\"] if item != \"sex\"]\n",
    "            problem[\"args\"][\"cat_features\"] = cflist\n",
    "\n",
    "\n",
    "    if problem[\"args\"]:\n",
    "        model = problem[\"model\"](**problem[\"args\"])\n",
    "    else:\n",
    "        model = problem[\"model\"]\n",
    "        \n",
    "    if is_fitted(model, X_test):\n",
    "        print(\"Model is already fitted!\")\n",
    "        return\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    metrics = compute_metrics(y_test, y_pred, problem)\n",
    "    fairness_metrics = compute_fairness_metrics(y_test, y_pred, problem, group_test)\n",
    "    metrics_return = metrics + fairness_metrics\n",
    "    return metrics_return, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(problems_classification, adult_dataset_generator, all_data, num_repeats = 1, num_folds = 2, protected_attributes = [\"sex\"], keep_protected_input=False):\n",
    "\n",
    "    average_problems = []\n",
    "    std_problems = []\n",
    "    for problem in problems_classification:\n",
    "        print(problem[\"model_name\"])\n",
    "\n",
    "        rkf = RepeatedKFold(n_splits=num_folds, n_repeats=num_repeats, random_state=42)\n",
    "        all_metrics_mean = []\n",
    "        all_metrics_std = []\n",
    "        metrics_all = []\n",
    "        for i, (train_index, test_index) in enumerate(rkf.split(all_data)):    \n",
    "\n",
    "            data_train, data_test = all_data.loc[train_index], all_data.loc[test_index]\n",
    "            data_train_encoded = adult_dataset_generator.encode(data_train, keep_dtypes=True)\n",
    "            data_test_encoded = adult_dataset_generator.encode(data_test)\n",
    "\n",
    "\n",
    "            X_train_real = data_train.copy().drop(columns=[\"income\"])\n",
    "            y_train_real = data_train_encoded[\"income\"].copy().astype(\"int\")\n",
    "\n",
    "            test_sets, _ = adult_dataset_generator.split_population(data_test)\n",
    "            test_sets[\"all\"] = data_test\n",
    "\n",
    "            split_dfs, additional_sizes = adult_dataset_generator.split_population(data_train, protected_attributes=protected_attributes)\n",
    "\n",
    "\n",
    "            # Get the DataFrame with the maximum length\n",
    "            max_length_df_key = max(split_dfs, key=lambda x: len(split_dfs[x]))\n",
    "            # Retrieve the DataFrame using the key\n",
    "            max_length_df = split_dfs[max_length_df_key]\n",
    "\n",
    "            max_length_df_class_counts = max_length_df['income'].value_counts()\n",
    "\n",
    "            max_length_df_majority_class = max_length_df_class_counts.idxmax()\n",
    "            max_length_df_majority_class_count = max_length_df_class_counts[max_length_df_majority_class]\n",
    "\n",
    "            augmented_dfs = []\n",
    "            split_df_keys, split_df_vals = zip(*split_dfs.items())\n",
    "\n",
    "\n",
    "            train_sets_X = [X_train_real]\n",
    "            train_sets_y = [y_train_real]\n",
    "\n",
    "            for generative_method in problem[\"generative_methods\"]:\n",
    "                for split_key, split_df in split_dfs.items():\n",
    "                    class_counts = split_df['income'].value_counts()\n",
    "                    augmented_dfs.append(split_df)\n",
    "\n",
    "                    for class_label, class_count in class_counts.items():\n",
    "                        minority_class_count = class_count\n",
    "                        imbalance = max_length_df_majority_class_count - minority_class_count\n",
    "                        size = imbalance\n",
    "\n",
    "                        if size > 0:\n",
    "                            class_split_df = split_df[split_df['income'] == class_label].copy()\n",
    "                            class_split_df.drop('income', axis=1, inplace=True)\n",
    "                            class_split_df.drop('sex', axis=1, inplace=True)\n",
    "                            split_synthesizer = adult_dataset_generator.train_synthesizer(generative_method, class_split_df, encode=True) \n",
    "                            split_synthetic_data = adult_dataset_generator.generate_data(split_synthesizer, num=size)\n",
    "                            split_synthetic_data['income'] = class_label\n",
    "                            split_synthetic_data['sex'] = split_key\n",
    "                            augmented_dfs.append(split_synthetic_data.copy())\n",
    "\n",
    "                augmented_trainingset = pd.concat(augmented_dfs)\n",
    "                augmented_trainingset_encoded = adult_dataset_generator.encode(augmented_trainingset, keep_dtypes=True)\n",
    "\n",
    "                X_train_augmented = augmented_trainingset.drop(columns=[\"income\"])\n",
    "                y_train_augmented = augmented_trainingset_encoded[\"income\"].astype(\"int\")\n",
    "\n",
    "                # train_real = data_train_encoded[\"income\"].astype(\"int\")\n",
    "                train_sets_X.append(X_train_augmented)\n",
    "                train_sets_y.append(y_train_augmented)\n",
    "\n",
    "            metrics_split = []\n",
    "            \n",
    "            for X_train, y_train in zip(train_sets_X, train_sets_y):\n",
    "                setup_metrics = []\n",
    "                preds = [] \n",
    "                for test_set_name, test_set in test_sets.items():\n",
    "                    test_set_encoded = adult_dataset_generator.encode(test_set)\n",
    "                    X_test = test_set.drop(columns=[\"income\"])\n",
    "                    y_test = test_set_encoded[\"income\"].astype(\"int\")\n",
    "                    results, pred = train_eval(X_train, y_train, X_test, y_test, problem, keep_protected_input=keep_protected_input)\n",
    "                    setup_metrics.append(results)\n",
    "                    preds.append(pred)\n",
    "                metrics_split.append(setup_metrics)\n",
    "            metrics_all.append(metrics_split)\n",
    "        metrics_all = np.array(metrics_all)    \n",
    "        average_metrics_all = np.mean(metrics_all, axis=0)\n",
    "        std_metrics_all = np.std(metrics_all, axis=0)\n",
    "        average_problems.append(average_metrics_all)\n",
    "        std_problems.append(std_metrics_all)\n",
    "    return np.array(average_problems), np.array(std_problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "problem_classification = {\"metrics\":[accuracy_score,  precision_score, recall_score, f1_score],\n",
    "                      \"metric_names\":[\"Accuracy\", \"P\", \"R\", \"F1\"],\n",
    "                      \"fairness_metrics\": [eq_odd],\n",
    "                      \"fairness_metric_names\": [\"Equalized odds\"],\n",
    "                      \"generative_methods\": [\"cart\", \"smote\"],}\n",
    "                      \n",
    "\n",
    "\n",
    "\n",
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "\n",
    "categorical_cols = adult_dataset_generator.categorical_input_cols.copy()\n",
    "categorical_cols.remove(\"sex\")\n",
    "\n",
    "transformations = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, adult_dataset_generator.continuous_input_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)])\n",
    "\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf_RF = Pipeline(steps=[('preprocessor', transformations),\n",
    "                      ('classifier', RandomForestClassifier(random_state=42))])\n",
    "clf_DT = Pipeline(steps=[('preprocessor', transformations),\n",
    "                    ('classifier', DecisionTreeClassifier(random_state=42))])     \n",
    "\n",
    "\n",
    "# models = [MultiOutputRegressor(LGBMRegressor(random_state=42)), DecisionTreeRegressor(random_state=42), RandomForestRegressor(random_state=42)]\n",
    "# models_classification = [xgb.XGBClassifier, CatBoostClassifier, DecisionTreeClassifier, RandomForestClassifier]\n",
    "# models_classification = [xgb.XGBClassifier]\n",
    "models_classification = [CatBoostClassifier, clf_DT, clf_RF]\n",
    "\n",
    "# args = [{\"random_state\":42}, {\"random_state\":42, \"loss_function\":\"MultiRMSE\", \"verbose\":False, \"iterations\":100, \"learning_rate\":0.01}, {\"random_state\":42}, {\"random_state\":42}]\n",
    "args = [{\"random_state\":42, \"loss_function\":\"Logloss\", \"verbose\":False, \"iterations\":100, \"learning_rate\":0.01, \"cat_features\":adult_dataset_generator.categorical_input_cols}, {}, {}]\n",
    "\n",
    "# model_names_classification = [\"xgboost\", \"catboost\", \"DT\", \"RF\"]\n",
    "model_names_classification = [\"Catboost\", \"Decission Tree\", \"Random Forest\"]\n",
    "problems_classification = []\n",
    "for model, name, arg in zip(models_classification, model_names_classification, args):\n",
    "    problem = problem_classification.copy()\n",
    "    problem[\"model\"] = copy.deepcopy(model)\n",
    "    problem[\"model_name\"] = name\n",
    "    problem[\"args\"] = arg\n",
    "    problems_classification.append(problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average, std = run_experiments(problems_classification, adult_dataset_generator, all_data, num_repeats = 5, num_folds = 3, protected_attributes = [\"sex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latex_table1(all_metrics_mean, all_metrics_std, names_train, names_test, problems, test_data=False, metric_names_actual=[]):\n",
    "    if test_data:\n",
    "        all_cols =  str(2 + len(metric_names_actual) * len(names_test))\n",
    "    else:\n",
    "        all_cols = str(len(problems[0][\"metric_names\"]) + 2)\n",
    "    latex_table = \"\\\\begin{table}[h]\\n\"\n",
    "    latex_table += \"\\\\centering\\n\"\n",
    "    # latex_table += \"\\\\scalebox{0.70}{\\n\"\n",
    "    latex_table += \"\\\\begin{tabular}{l l \" + \" \".join([\"c\"]*(int(all_cols)-2)) + \"}\\n\"\n",
    "    latex_table += \"\\\\hline\\n\"\n",
    "    if test_data:\n",
    "        if len(metric_names_actual) > 0:\n",
    "            latex_table += \"Model & Train data & \\multicolumn{\" + str(len(names_test) * len(metric_names_actual)) + \"}{c}{Test data} \\\\\\\\\\n\" \n",
    "            latex_table += \"& \"\n",
    "            for name_t in names_test:\n",
    "                latex_table +=\" & \\multicolumn{\" + str(len(metric_names_actual)) + \"}{c}{\" + name_t + \"}\"\n",
    "            latex_table += \"\\\\\\\\\\n\"\n",
    "            latex_table += \"\\cline{3-\" + str(all_cols) +\"}\"\n",
    "            # latex_table +=  \"& & \" + \" & \".join(metric_names_actual) + \" & \" + \" & \".join(metric_names_actual) + \" \\\\\\\\\\n\"\n",
    "            latex_table +=  \"& \" + \"\".join([\" & \" + \" & \".join(metric_names_actual) for _ in range(len(names_test))]) + \" \\\\\\\\\\n\"\n",
    "        else:\n",
    "            latex_table += \"Model & Train data & Test data & \" + \" & \".join(problems[0][\"metric_names\"]) + \" \\\\\\\\\\n\"\n",
    "    else:\n",
    "        if len(metric_names_actual) > 0:\n",
    "            latex_table += \"Model & Train data & \" + \" & \".join(metric_names_actual) + \" \\\\\\\\\\n\"\n",
    "        else:\n",
    "            latex_table += \"Model & Train data & \" + \" & \".join(problems[0][\"metric_names\"]) + \" \\\\\\\\\\n\"\n",
    "\n",
    "    latex_table += \"\\\\hline\"\n",
    "    count_make_cell = sum(\"makecell\" in item for item in names_train)\n",
    "\n",
    "    for problem_i in range(len(problems)):\n",
    "        print(problems[problem_i][\"model_name\"])\n",
    "        latex_table += \"\\\\multirow{\" + str(2*len(problems)) + \"}{*}{\" + problems[problem_i][\"model_name\"] + \"}\"\n",
    "\n",
    "        for i in range(len(names_train)):\n",
    "            train_name = names_train[i]\n",
    "            # if \"makecell\" in train_name:\n",
    "            #     latex_table += \" & \" + \"\\\\multirow{2}{*}{\" + train_name + \"}\"\n",
    "            # else:\n",
    "            latex_table += \" & \" + \"\\\\multirow{2}{*}{\" + train_name + \"}\"\n",
    "            # avg_metric = all_metrics_mean[metric_row][name_row][metric_col]\n",
    "            # std_metric = all_metrics_std[metric_row][name_row][metric_col]\n",
    "            # latex_table += f\"& {avg_metric:.3f} ({std_metric:.3f})\"\n",
    "            avgs_c = \"\"\n",
    "            stds_c = \"\"\n",
    "            for j in range(len(names_test)):\n",
    "                test_name = names_test[j]\n",
    "                avg_metric = all_metrics_mean[problem_i][i][j]\n",
    "                std_metric = all_metrics_std[problem_i][i][j]\n",
    "                # std_metric = all_metrics_std[metric_row][name_row][metric_col]\n",
    "                avgs_c += \" & \" + \" & \".join(map(lambda x: \"{:.3f}\".format(x), avg_metric))\n",
    "                stds_c += \" & \" + \" & \".join(map(lambda x: \"({:.3f})\".format(x), std_metric))\n",
    "\n",
    "                # if test_data:\n",
    "                #     latex_table += \" & \" + test_name + \" & \" +  numbers + \" \\\\\\\\\\n\"\n",
    "                # else:\n",
    "            latex_table += avgs_c + \" \\\\\\\\\\n\"\n",
    "            latex_table += \" & \" + stds_c + \" \\\\\\\\\\n\"\n",
    "\n",
    "                # latex_table += \"\\\\cline{2-\" + all_cols + \"}\\n\"\n",
    "        latex_table += \"\\\\hline\\n\"\n",
    "    latex_table += \"\\\\end{tabular}\\n\"\n",
    "    # latex_table += \"}\\n\"\n",
    "    latex_table += \"\\\\caption{Comparison}\\n\"\n",
    "    latex_table += \"\\\\label{tab:eval}\\n\"\n",
    "    latex_table += \"\\\\end{table}\"\n",
    "    \n",
    "    return latex_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(problems_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names_actual = [\"Accuracy\", \"P\", \"R\", \"F1\", \"Equalized Odds\"]\n",
    "names_train = [\"Adult\", \"Augmented Adult (CART)\", \"Augmented Adult (SMOTENC)\"]\n",
    "test_sets, _ = adult_dataset_generator.split_population(all_data)\n",
    "protected_attributes = [\"Sex\"]\n",
    "# names_test = [\"\\\\makecell[c]{\" + '\\\\\\\\'.join([f\"{attr}-{value}\" for attr, value in zip(protected_attributes, entry)]) + \"}\" for entry in test_sets.keys()]\n",
    "# names_test = [' \\& '.join([f\"{attr}={value}\" for attr, value in zip(protected_attributes, entry)]) for entry in test_sets.keys()]\n",
    "names_test = [f\"Sex={value}\" for value in test_sets.keys()]\n",
    "names_test.append(\"Overall\")\n",
    "latex_table = generate_latex_table1(average, std, names_train, names_test, problems_classification, metric_names_actual=metric_names_actual, test_data=True)\n",
    "# print(latex_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samplestructures",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
